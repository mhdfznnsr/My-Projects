{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4354a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ed3601dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "cc9f7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9def8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Articles from URLs and saving into .txt file\n",
    "\n",
    "for i in range(0,170):\n",
    "    content = \"\"\n",
    "    file_name = str(i+1)+\".txt\"\n",
    "    url = input['URL'][i]\n",
    "    page = requests.get(url,headers={\"User-Agent\": \"XY\"})    \n",
    "    soup = BeautifulSoup(page.content,'html.parser')    \n",
    "    title = soup.findAll('h1',attrs = {'class':'entry-title'})\n",
    "    title = title[0].text\n",
    "    article = soup.findAll(attrs = {'class':'td-post-content'})\n",
    "    text_article = [t for t in article[0].select(':not(pre)')]\n",
    "    content = \"\"\n",
    "    for tag in text_article:\n",
    "        content = content + tag.text\n",
    "    content = content.replace(u'\\xa0', u' ')\n",
    "    with open(file_name,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "90ecbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Methods to get values of attributes\n",
    "\n",
    "def sentiment_analysis(text,cleaned_text):\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "    polarity_score = (score['pos'] - score['neg'])/((score['pos']+score['neg'])+0.000001)\n",
    "    subjectivity_score = (score['pos'] + score['neg'])/(len(cleaned_text)+0.000001)\n",
    "    return [score['pos'],score['neg'],polarity_score,subjectivity_score]\n",
    "\n",
    "def readability(word_count,sentences,complex_words):\n",
    "    avg_sen_len = word_count/sentences\n",
    "    percent_Complex_Words = complex_words/word_count\n",
    "    fog_index = 0.4*(avg_sen_len+percent_Complex_Words)\n",
    "    words_per_sen = word_count/sentences\n",
    "    return [avg_sen_len,percent_Complex_Words,fog_index,words_per_sen]\n",
    "\n",
    "#Syllable Count\n",
    "\n",
    "def sylco(word) :\n",
    "    word = word.lower()\n",
    "\n",
    "    # exception_add are words that need extra syllables\n",
    "    # exception_del are words that need less syllables\n",
    "\n",
    "    exception_add = ['serious','crucial']\n",
    "    exception_del = ['fortunately','unfortunately']\n",
    "\n",
    "    co_one = ['cool','coach','coat','coal','count','coin','coarse','coup','coif','cook','coign','coiffe','coof','court']\n",
    "    co_two = ['coapt','coed','coinci']\n",
    "\n",
    "    pre_one = ['preach']\n",
    "\n",
    "    syls = 0 #added syllable number\n",
    "    disc = 0 #discarded syllable number\n",
    "\n",
    "    #1) if letters < 3 : return 1\n",
    "    if len(word) <= 3 :\n",
    "        syls = 1\n",
    "        return syls\n",
    "\n",
    "    #2) if doesn't end with \"ted\" or \"tes\" or \"ses\" or \"ied\" or \"ies\", discard \"es\" and \"ed\" at the end.\n",
    "    # if it has only 1 vowel or 1 set of consecutive vowels, discard. (like \"speed\", \"fled\" etc.)\n",
    "\n",
    "    if word[-2:] == \"es\" or word[-2:] == \"ed\" :\n",
    "        doubleAndtripple_1 = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "        if doubleAndtripple_1 > 1 or len(re.findall(r'[eaoui][^eaoui]',word)) > 1 :\n",
    "            if word[-3:] == \"ted\" or word[-3:] == \"tes\" or word[-3:] == \"ses\" or word[-3:] == \"ied\" or word[-3:] == \"ies\" :\n",
    "                pass\n",
    "            else :\n",
    "                disc+=1\n",
    "\n",
    "    #3) discard trailing \"e\", except where ending is \"le\"  \n",
    "\n",
    "    le_except = ['whole','mobile','pole','male','female','hale','pale','tale','sale','aisle','whale','while']\n",
    "\n",
    "    if word[-1:] == \"e\" :\n",
    "        if word[-2:] == \"le\" and word not in le_except :\n",
    "            pass\n",
    "\n",
    "        else :\n",
    "            disc+=1\n",
    "\n",
    "    #4) check if consecutive vowels exists, triplets or pairs, count them as one.\n",
    "\n",
    "    doubleAndtripple = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "    tripple = len(re.findall(r'[eaoui][eaoui][eaoui]',word))\n",
    "    disc+=doubleAndtripple + tripple\n",
    "\n",
    "    #5) count remaining vowels in word.\n",
    "    numVowels = len(re.findall(r'[eaoui]',word))\n",
    "\n",
    "    #6) add one if starts with \"mc\"\n",
    "    if word[:2] == \"mc\" :\n",
    "        syls+=1\n",
    "\n",
    "    #7) add one if ends with \"y\" but is not surrouned by vowel\n",
    "    if word[-1:] == \"y\" and word[-2] not in \"aeoui\" :\n",
    "        syls +=1\n",
    "\n",
    "    #8) add one if \"y\" is surrounded by non-vowels and is not in the last word.\n",
    "\n",
    "    for i,j in enumerate(word) :\n",
    "        if j == \"y\" :\n",
    "            if (i != 0) and (i != len(word)-1) :\n",
    "                if word[i-1] not in \"aeoui\" and word[i+1] not in \"aeoui\" :\n",
    "                    syls+=1\n",
    "\n",
    "    #9) if starts with \"tri-\" or \"bi-\" and is followed by a vowel, add one.\n",
    "\n",
    "    if word[:3] == \"tri\" and word[3] in \"aeoui\" :\n",
    "        syls+=1\n",
    "\n",
    "    if word[:2] == \"bi\" and word[2] in \"aeoui\" :\n",
    "        syls+=1\n",
    "\n",
    "    #10) if ends with \"-ian\", should be counted as two syllables, except for \"-tian\" and \"-cian\"\n",
    "\n",
    "    if word[-3:] == \"ian\" : \n",
    "    #and (word[-4:] != \"cian\" or word[-4:] != \"tian\") :\n",
    "        if word[-4:] == \"cian\" or word[-4:] == \"tian\" :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #11) if starts with \"co-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly.\n",
    "\n",
    "    if word[:2] == \"co\" and word[2] in 'eaoui' :\n",
    "\n",
    "        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two :\n",
    "            syls+=1\n",
    "        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #12) if starts with \"pre-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly.\n",
    "\n",
    "    if word[:3] == \"pre\" and word[3] in 'eaoui' :\n",
    "        if word[:6] in pre_one :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #13) check for \"-n't\" and cross match with dictionary to add syllable.\n",
    "\n",
    "    negative = [\"doesn't\", \"isn't\", \"shouldn't\", \"couldn't\",\"wouldn't\"]\n",
    "\n",
    "    if word[-3:] == \"n't\" :\n",
    "        if word in negative :\n",
    "            syls+=1\n",
    "        else :\n",
    "            pass   \n",
    "\n",
    "    #14) Handling the exceptional words.\n",
    "\n",
    "    if word in exception_del :\n",
    "        disc+=1\n",
    "\n",
    "    if word in exception_add :\n",
    "        syls+=1     \n",
    "\n",
    "    # calculate the output\n",
    "    return numVowels - disc + syls\n",
    "\n",
    "#complex words count\n",
    "\n",
    "def complex_word_count(word_list):\n",
    "    complex_words = len(list(filter(lambda word: sylco(word) > 2,word_list)))\n",
    "    return complex_words\n",
    "\n",
    "#finding pronouns\n",
    "\n",
    "def pronouns(text):\n",
    "    pattern = re.compile(r'I|me|mine|we|my|ours|us|our')\n",
    "    count = 0\n",
    "    matches = pattern.finditer(text)\n",
    "    for match in matches:\n",
    "        count = count+1\n",
    "    return count\n",
    "\n",
    "#average word length\n",
    "\n",
    "def avgWordLen(cleaned_text):\n",
    "    words = len(cleaned_text)\n",
    "    chars = [len(word) for word in cleaned_text]\n",
    "    chars = sum(chars)\n",
    "    return chars/words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "75417d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Words\n",
    "\n",
    "stop_words = open(\"StopWords.txt\",encoding = \"utf-8\").read()\n",
    "stop_words = stop_words.split('\\n')\n",
    "\n",
    "output = pd.read_csv('output.csv')\n",
    "cols = ['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH',\n",
    "        'PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT',\n",
    "        'SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "for i in range(0,170):\n",
    "    final_values = []\n",
    "    file_name = str(i+1)+\".txt\"\n",
    "    text = open(file_name,encoding = \"utf-8\").read()\n",
    "    text_lower = text.lower()\n",
    "    sentences = len(sent_tokenize(text_lower))\n",
    "    #cleaning punctuations and stop words\n",
    "    \n",
    "    punctuation_free = text_lower.translate(str.maketrans('','',string.punctuation))\n",
    "    punctuation_free_list = punctuation_free.split()\n",
    "    cleaned_text = []\n",
    "\n",
    "    #removing stop words\n",
    "\n",
    "    for word in punctuation_free_list:\n",
    "        if word not in stop_words:\n",
    "            cleaned_text.append(word)\n",
    "            \n",
    "    #positive score, negative score, polarity score, subjectivity score\n",
    "    \n",
    "    sentiment = sentiment_analysis(text_lower,cleaned_text)\n",
    "    complex_words = complex_word_count(cleaned_text)\n",
    "    word_count = len(cleaned_text)\n",
    "    read = readability(word_count,sentences,complex_words)\n",
    "    syllables = sum([sylco(word) for word in cleaned_text])/word_count\n",
    "    personal_pronouns = pronouns(text_lower)\n",
    "    avg_word_len = avgWordLen(cleaned_text)\n",
    "    final_values = [sentiment,read,complex_words,word_count,syllables,personal_pronouns,avg_word_len]\n",
    "    final_values = list(np.hstack(final_values))\n",
    "    output.loc[i,cols] = final_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "62ccf5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('final_output.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa993f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
